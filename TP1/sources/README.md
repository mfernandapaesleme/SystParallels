
# TD1

`pandoc -s --toc README.md --css=./github-pandoc.css -o README.html`

## lscpu

*lscpu donne des infos utiles sur le processeur : nb core, taille de cache :*

```
Architecture:            x86_64
  CPU op-mode(s):        32-bit, 64-bit
  Address sizes:         39 bits physical, 48 bits virtual
  Byte Order:            Little Endian

CPU(s):                  4
  On-line CPU(s) list:   0-3

Vendor ID:               GenuineIntel
  Model name:            Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz
    CPU family:          6
    Model:               142
    Thread(s) per core:  2
    Core(s) per socket:  2
    Socket(s):           1
    Stepping:            9
    BogoMIPS:            5807.99
   
Caches (sum of all):     
  L1d:                   64 KiB (2 instances)
  L1i:                   64 KiB (2 instances)
  L2:                    512 KiB (2 instances)
  L3:                    4 MiB (1 instance)

`````````````````````````````````````````````


## Produit matrice-matrice

### Effet de la taille de la matrice

  n            | MFlops    | Temps CPU
---------------|-----------|--------
1024 (origine) | 59.0197   | 36.3859 s
1023           | 92.5256   | 23.1417 s
1025           | 90.2636   | 23.861  s
1026           | 96.1386   | 22.4685 s
2047           | 100.759   | 170.256 s
2048           | 73.2614   | 234.501 s
2049           | 102.703   | 167.523 s

*RÃ©sultats*
La diffÃ©rence de performance entre les tailles de matrices multiples de 2 et celles qui ne le sont pas peut s'expliquer par la gestion de la mÃ©moire par le CPU et la structure mÃªme de la mÃ©moire.

Les architectures modernes de processeurs sont conÃ§ues pour stocker et accÃ©der aux donnÃ©es de maniÃ¨re sÃ©quentielle. Lorsqu'une variable est utilisÃ©e Ã  la position ğ‘–, il est frÃ©quent d'accÃ©der ensuite Ã  la position ğ‘– + 1. Ainsi, le CPU optimise ces accÃ¨s en enregistrant les variables en sÃ©quence.

La mÃ©moire est construite Ã  partir de structures binaires et fonctionne avec des tailles multiples de 2. Lorsque la taille de la matrice est une puissance de 2, lâ€™adressage peut entraÃ®ner des conflits de cache car certaines donnÃ©es sont mappÃ©es aux mÃªmes emplacements mÃ©moire, ce qui force le CPU Ã  recharger les donnÃ©es plus souvent et entraÃ®ne une perte d'efficacitÃ©.

Cela explique pourquoi, pour ğ‘› = 1024, la performance est plus faible en raison de ces conflits. En revanche, pour ğ‘› = 1023, 1025, 1026, qui ne sont pas des puissances de 2, lâ€™accÃ¨s Ã  la mÃ©moire est plus efficace, rÃ©duisant les conflits de cache et amÃ©liorant lâ€™utilisation des ressources du CPU. Ainsi, les performances en MFlops sont meilleures et le temps dâ€™exÃ©cution est rÃ©duit.

Pour ğ‘› = 2048, le problÃ¨me est diffÃ©rent : la taille de la matrice dÃ©passe la capacitÃ© du cache L3 du processeur (4 MiB). Comme une matrice 2048 Ã— 2048 en double prÃ©cision occupe 32 MiB, la majoritÃ© des accÃ¨s mÃ©moire se fait en RAM, qui est beaucoup plus lente que le cache. Cela entraÃ®ne une forte augmentation du temps dâ€™exÃ©cution et une diminution des MFlops.

En rÃ©sumÃ©, la performance dÃ©pend de lâ€™alignement mÃ©moire et de lâ€™utilisation efficace du cache. Les tailles qui minimisent les conflits de cache et qui restent dans les limites du cache L3 offrent de meilleures performances.

### Permutation des boucles

*Expliquer comment est compilÃ© le code (ligne de make ou de gcc) : on aura besoin de savoir l'optim, les paramÃ¨tres, etc. Par exemple :*

`make TestProduct.exe && ./TestProduct.exe 1024`


  ordre           | time    | MFlops  | MFlops(n=2048)
------------------|---------|---------|----------------
i,j,k (origine)   | 5.97182 | 359.603 | 165.088
j,i,k             | 9.63033 | 222.992 | 167.944
i,k,j             | 27.8432 | 77.1278 | 66.5025
k,i,j             | 28.4341 | 75.5248 | 66.2263
j,k,i             | 0.99214 | 2164.5  | 2038.18
k,j,i             | 2.50896 | 855.925 | 1150.28


*RÃ©sultats.*
On remarque que l'ordre des boucles a un impact significatif sur la performance. L'ordre j,k,i est le plus performant avec 2164.5 MFlops pour n=1024, tandis que k,i,j et i,k,j sont les moins performants avec environ 75-77 MFlops.

La raison principale est la maniÃ¨re dont les donnÃ©es sont stockÃ©es et accÃ©dÃ©es en mÃ©moire. Dans l'architecture mÃ©moire moderne, les donnÃ©es sont stockÃ©es en groupe, gÃ©nÃ©ralement par lignes. Lorsque l'itÃ©ration la plus frÃ©quente est effectuÃ©e sur i, cela signifie que l'accÃ¨s mÃ©moire est sÃ©quentiel au sein d'une ligne, ce qui amÃ©liore la localitÃ© spatiale et rÃ©duit les accÃ¨s redondants au cache.

Dans le meilleure code (jki), i reprÃ©sente les lignes et j les colonnes. Lorsque l'itÃ©ration principale se fait sur i, les accÃ¨s mÃ©moire suivent un schÃ©ma optimisÃ©, limitant les conflits de cache et amÃ©liorant les performances globales.



### OMP sur la meilleure boucle

`make TestProductMatrix.exe && OMP_NUM_THREADS=8 ./TestProduct.exe 1024`

  OMP_NUM         | MFlops  | MFlops(n=2048) | MFlops(n=512)  | MFlops(n=4096)
------------------|---------|----------------|----------------|---------------
          1       | 2205.05 |    1718.21     |     1958.75    |  1861.64
          2       | 2147.67 |    2169.58     |     3402.88    |  2566.45
          3       | 3136.89 |    2498.59     |     3048.99    |  2403.1
          4       | 3650.08 |    2545.44     |     2662.67    |  2322.51                                
          5       | 2557.92 |    2856.77     |     3339.79    |  2389.14
          6       | 3470.06 |    2649.2      |     3721.35    |  1994.97
          7       | 2677.08 |    2633.02     |     3635.96    |  2190.33
          8       | 3470.28 |    2789.76     |     3102.2     |  2742.43

*Tracer les courbes de speedup (pour chaque valeur de n), discuter les rÃ©sultats.*

![alt text](image.png)

L'ajout de l'OpenMP amÃ©liore les performances en parallÃ¨le, mais avec certaines limitations. Lorsque OMP_NUM_THREADS=1, la performance atteint environ 2205.05 MFlops pour n=1024. En augmentant le nombre de threads Ã  8, la performance augmente Ã  3470.28 MFlops.

Cependant, on observe qu'aprÃ¨s un certain nombre de threads (entre 4 et 6), la performance commence Ã  fluctuer et ne s'amÃ©liore plus systÃ©matiquement. Cela peut Ãªtre dÃ» Ã  une surcharge de synchronisation entre les threads, un partage inefficace des ressources du cache entre les threads ou une saturation des unitÃ©s de calcul du CPU.

Ainsi, bien que l'OpenMP amÃ©liore considÃ©rablement la performance, il existe une limite au-delÃ  de laquelle l'ajout de threads n'apporte plus de bÃ©nÃ©fice.



### Produit par blocs

`make TestProduct.exe && ./TestProduct.exe 1024`

  szBlock         | MFlops  | MFlops(n=2048) | MFlops(n=512)  | MFlops(n=4096)
------------------|---------|----------------|----------------|---------------
origine (=max)    |  
32                | 1562.28 |    980.154     |    3047.65     |   810.341
64                | 1692.82 |    1097.51     |    3527.45     |   909.33
128               | 2752.01 |    1515.26     |    3227.92     |   1189
256               | 1724.25 |    1961.06     |    2508.5      |   1593.53
512               | 2490.69 |    1739.55     |    2167.51     |   2324.04
1024              | 1647.02 |    2098.8      |  erreur numer  |   2362.12

*RÃ©sultats.*
Les rÃ©sultats montrent comment les performances (MFlops) varient en fonction de la taille des blocs (szBlock) et de diffÃ©rentes tailles de donnÃ©es (n).

Petits blocs (szBlock = 32, 64) : Les performances sont faibles, car des blocs trop petits n'exploitent pas bien le cache du processeur.
Blocs moyens (szBlock = 128, 256) : Meilleures performances, surtout avec szBlock = 128, qui optimise l'utilisation du cache et l'accÃ¨s Ã  la mÃ©moire.
Grands blocs (szBlock = 512, 1024) : Les performances commencent Ã  baisser avec des blocs plus grands, car trop de donnÃ©es peuvent saturer le cache et devenir inefficaces.

Erreur numÃ©rique (szBlock = 1024, n = 512) : Cette erreur indique que l'utilisation de blocs trop grands pour des tailles de donnÃ©es plus petites cause des problÃ¨mes de calcul.

De cette maniÃ¨re, on peut concluire que les tailles de blocs moyennes (128 Ã  512) offrent le meilleur compromis entre performance et efficacitÃ©. Des blocs trop petits ou trop grands peuvent rÃ©duire les performances.



### Bloc + OMP


  szBlock      | OMP_NUM | MFlops  | MFlops(n=2048) | MFlops(n=512)  | MFlops(n=4096)|
---------------|---------|---------|----------------|----------------|---------------|
      1024     |  1      | 1927.51 |    1952.15     |   erreur num   |    1758.14    |
      1024     |  8      | 1668.04 |    2620.97     |   erreur num   |    2403.91    |
      512      |  1      | 1677.29 |    1363.72     |    1598.8      |    1637.81    |
      512      |  8      | 1597.13 |    2736.25     |    1622.84     |    1815.29    |

*RÃ©sultats.*
Les rÃ©sultats montrent que l'utilisation d'OpenMP amÃ©liore les performances pour les grandes tailles de matrices, surtout avec 8 threads. Avec szBlock = 1024, l'ajout de threads amÃ©liore les performances pour n = 2048 et 4096, mais reste infÃ©rieur Ã  la version mono-thread pour n = 1024. Avec szBlock = 512, l'effet du parallÃ©lisme est plus marquÃ©, notamment pour n = 2048 oÃ¹ l'on observe une nette augmentation du MFlops.

Cependant, il y a une erreur numÃ©rique pour n = 512 avec szBlock = 1024, ce qui suggÃ¨re que l'utilisation de blocs trop grands avec un nombre rÃ©duit de donnÃ©es peut poser des problÃ¨mes de prÃ©cision ou d'alignement mÃ©moire.


### Comparaison avec BLAS, Eigen et numpy

*Comparer les performances avec un calcul similaire utilisant les bibliothÃ¨ques d'algÃ¨bre linÃ©aire BLAS, Eigen et/ou numpy.*

ComparÃ© aux bibliothÃ¨ques comme BLAS, Eigen et Numpy, l'optimisation de bas niveau de BLAS permet gÃ©nÃ©ralement d'obtenir de meilleures performances grÃ¢ce Ã  des optimisations spÃ©cifiques au processeur. Eigen offre un bon compromis en C++ avec des optimisations similaires, tandis que Numpy, bien que plus accessible, peut Ãªtre moins performant pour des calculs lourds sans l'utilisation de BLAS en arriÃ¨re-plan.


# Tips

```
	env
	OMP_NUM_THREADS=4 ./produitMatriceMatrice.exe
```

```
    $ for i in $(seq 1 4); do elap=$(OMP_NUM_THREADS=$i ./TestProductOmp.exe|grep "Temps CPU"|cut -d " " -f 7); echo -e "$i\t$elap"; done > timers.out
```
